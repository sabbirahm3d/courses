{"cells":[{"cell_type":"markdown","source":["# Lesson 11 - Starter Code"],"metadata":{}},{"cell_type":"markdown","source":["### Part 1: Intro to Databricks Notebook\n\nThis notebook is very similar to an IPython Notebook. Cells can have markdown or code and are attached to a cluster where the code is executed in a distributed envrionment."],"metadata":{}},{"cell_type":"code","source":["print \"hello, world!\""],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":["A SparkContext (`sc`) is the entry point to Spark for a Spark application.  \nSimilarly, a SQLContext (`sqlContext`) is the entry point to a Spark SQL DB.  \nRead more at the [Spark Docs](http://spark.apache.org/docs/latest/)"],"metadata":{}},{"cell_type":"code","source":["# A Spark Context is already created for you.\n# Do not create another or unspecified behavior may occur.\nsc"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["# A SQLContext is also already created for you.\n# Do not create another or unspecified behavior may occur.\nsqlContext"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":["There are a large number of Databricks hosted datasets available for you.  \nSee the full [List of Databricks Hosted Datasets available here](https://docs.cloud.databricks.com/docs/latest/databricks_guide/index.html#03%20Data%20Sources/6%20Databricks%20Public%20Datasets/1%20DBFS%20Hosted%20Datasets.html)"],"metadata":{}},{"cell_type":"code","source":["%fs ls /databricks-datasets"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":["### Part 2: (DEMO) Word Count Example"],"metadata":{}},{"cell_type":"markdown","source":["Load data in from a text file into a Resilient Distributed Dataset (RDD).  \nAn RDD is the main data structure abstraction in Spark and provides a collection of elements partitioned across the nodes of the cluster and can be operated on in parallel."],"metadata":{}},{"cell_type":"code","source":["# Read a text file into an RDD\ndataRDD = sc.textFile(\"dbfs:/databricks-datasets/cs100/lab1/data-001/shakespeare.txt\")\n\nprint \"Data type of dataRDD:\"\nprint type(dataRDD)\n\nprint \"Number of elements (lines) in dataRDD:\"\nprint dataRDD.count()\n\nprint \"First ten elements (lines) in dataRDD:\"\nprint dataRDD.take(10)"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["# RDD transformations\nword_counts = dataRDD \\\n    .flatMap(lambda line: line.split(\" \")) \\\n    .map(lambda word: (word, 1)) \\\n    .reduceByKey(lambda a, b: a + b)"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":["# RDD actions\nword_counts.collect()"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"code","source":["# Get top ten words\nword_counts.takeOrdered(10, key=lambda x: -x[1])"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":["### Part 3: (GUIDED PRACTICE) Word Length Count\n\nPerform the word count as above, but add an additional transformation to convert each word to its length."],"metadata":{}},{"cell_type":"code","source":["# RDD transformations\nword_len_counts = dataRDD \\\n    .flatMap(lambda line: line.split(\" \")) \\\n    .map(### FILL IN ###) \\\n    .map(lambda word_len: (word_len, 1)) \\\n    .reduceByKey(lambda a, b: a + b)"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"code","source":["# RDD actions\nword_len_counts.### FILL IN ###"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"code","source":["# Get top ten word lengths\nword_len_counts.### FILL IN ###"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"code","source":["# RDD transformations to filter long words\nword_lengths = dataRDD \\\n    .flatMap(lambda line: line.split(\" \")) \\\n    .map(lambda word: (word, len(word))) \\\n    .filter(lambda x: x[1]>20)"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"code","source":["# RDD actions\nword_lengths.collect()"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"code","source":["# Get top ten longest words\nword_lengths.### FILL IN ###"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"markdown","source":["### Part 4: (DEMO) Spark DataFrame"],"metadata":{}},{"cell_type":"markdown","source":["#### 4.1 Load data"],"metadata":{}},{"cell_type":"code","source":["# Read titanic data as DataFrame using spark-csv package, and cache it\ntitanic = sqlContext.read.format('com.databricks.spark.csv').options(header='true', inferSchema='true').load('/databricks-datasets/Rdatasets/data-001/csv/COUNT/titanic.csv').cache()\n\nprint \"Data type of titanic:\"\nprint type(titanic)\n\nprint \"Number of elements (records) in titanic:\"\nprint titanic.count()\n\nprint \"First ten elements (records) in titanic:\"\nprint titanic.head(10)"],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"markdown","source":["#### 4.2 Explore data"],"metadata":{}},{"cell_type":"code","source":["display(titanic.where(\"class like '1st class' and age like 'child'\"))"],"metadata":{},"outputs":[],"execution_count":26},{"cell_type":"markdown","source":["#### 4.3 Pre-process data\n\nIn this section, we convert string categorical columns into ordered indices usable by a linear model. Note that these categorical features are special: There is a natural ordering, so it makes sense to treat them as continuous. For general categorical features, you should probably use one-hot encoding instead."],"metadata":{}},{"cell_type":"code","source":["# Compute lists of string categories\ndef getCategories(col):\n  vals = sorted(titanic.select(col).distinct().rdd.map(lambda x: x[0]).collect())\n  valDict = dict([(vals[i], i) for i in range(len(vals))])\n  print col + ': ' + ', '.join(vals)\n  return (vals, valDict)\n\n(classes, classDict) = getCategories(\"class\")\n(ages, ageDict) = getCategories(\"age\")\n(sexes, sexDict) = getCategories(\"sex\")\n(survived, survivedDict) = getCategories(\"survived\")"],"metadata":{},"outputs":[],"execution_count":28},{"cell_type":"code","source":["# Convert the string categories into indices\nfrom pyspark.sql.types import *\n\nclassUDF = udf(lambda x: classDict[x], IntegerType())\nageUDF = udf(lambda x: ageDict[x], IntegerType())\nsexUDF = udf(lambda x: sexDict[x], IntegerType())\nsurvivedUDF = udf(lambda x: survivedDict[x], IntegerType())\n\ntitanicIndexed = titanic.select(classUDF(titanic[\"class\"]).alias(\"class\"), ageUDF(titanic[\"age\"]).alias(\"age\"), sexUDF(titanic[\"sex\"]).alias(\"sex\"), survivedUDF(titanic[\"survived\"]).alias(\"survived\")).cache()\n\ndisplay(titanicIndexed.describe())"],"metadata":{},"outputs":[],"execution_count":29},{"cell_type":"markdown","source":["#### 4.4 Train a model\n\nWe now train a Logistic Regression model using `LogisticRegressionWithSGD`. Since we will use the traditional MLlib API (not the Pipelines API), we first have to extract the label and features columns and create an RDD of LabeledPoints. (The Pipelines API takes DataFrames instead of RDDs.)"],"metadata":{}},{"cell_type":"code","source":["# Convert data to RDD of LabeledPoint\nfrom pyspark.mllib.regression import LabeledPoint\n\nfeatureCols = [\"age\", \"sex\", \"class\"]\ntitanicLabels = titanicIndexed.select(\"survived\").map(lambda row: row[0])\ntitanicFeatures = titanicIndexed.select(*featureCols).map(lambda x: list(x)) #[x[0], x[1], x[2]])\ntitanicData = titanicLabels.zip(titanicFeatures).map(lambda l_p: LabeledPoint(l_p[0], l_p[1])).cache()"],"metadata":{},"outputs":[],"execution_count":31},{"cell_type":"code","source":["# Train the model, and print the intercept and weight vector\n# We use L1 (sparsifying) regularization, but you can also use None or \"l2\".\nfrom pyspark.mllib.classification import LogisticRegressionWithSGD\n\nlr = LogisticRegressionWithSGD.train(titanicData, regParam=0.1, regType=\"l1\", intercept=True, iterations=100)"],"metadata":{},"outputs":[],"execution_count":32},{"cell_type":"markdown","source":["#### 4.5 Evaluate the model"],"metadata":{}},{"cell_type":"code","source":["# We can make a single prediction:\noneInstance = [0, 1, 0]\nprediction = lr.predict(oneInstance)\nprint 'Example prediction:'\nprint '  features: ' + str(oneInstance)\nprint '  prediction: %d' % prediction"],"metadata":{},"outputs":[],"execution_count":34},{"cell_type":"code","source":["# We can also make predictions on the whole dataset and compute accuracy\nimport numpy\n\ndef accuracy(model, labelsRDD, featuresRDD):\n  predictionsRDD = featuresRDD.map(lambda x: model.predict(x))\n  return labelsRDD.zip(predictionsRDD).map(lambda labelAndPred: labelAndPred[0] == labelAndPred[1]).mean()\n\nprint 'Training accuracy: %g' % accuracy(lr, titanicLabels, titanicFeatures)"],"metadata":{},"outputs":[],"execution_count":35},{"cell_type":"code","source":["# Previously, we were making 0/1 predictions.  We can clear the model threshold to make soft predictions.\n# Note: Soft prediction are currently only supported for binary classification.\nlr.clearThreshold()\nprint 'Predicted probability of label 1: %g' % lr.predict(oneInstance)"],"metadata":{},"outputs":[],"execution_count":36},{"cell_type":"markdown","source":["Read the Spark MLlib guide to get complete list of ML algorithms in Spark:  \n[Spark Machine Learning Library (MLlib) Guide](http://spark.apache.org/docs/latest/mllib-guide.html)"],"metadata":{}},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":38}],"metadata":{"name":"starter-code-11","notebookId":2277307958881729},"nbformat":4,"nbformat_minor":0}
