%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% Sabbir Ahmed
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass[paper=usletter, fontsize=12pt]{article}
\input{../structure.tex} % specifies the document layout and style

\begin{document}

    \documentinfo{\today}{12}

    \begin{itemize}

        \item[\textbf{4.1}]
        \begin{enumerate}

            \item[\textbf{1}] Let $f(x)$, $g(x)$, $g(x) \in F[x]$. Show that
            the following properties hold.
            \begin{enumerate}

                \item[\textbf{c}] If $g(x) \mid f(x)$, then $g(x) \cdot h(x)
                \mid f(x) \cdot h(x)$.
                \begin{proof}

                    Since $g(x) \mid f(x)$, then by definition $f(x)=q(x)g(x)$,
                    $q(x) \in F[x]$\\
                    Then,
                    \begin{align*}
                        f(x)h(x) &= q(x)g(x)h(x)\\
                        & = q(x)(g(x)h(x))\\
                        \implies & g(x)h(x) \mid f(x)h(x) \qedhere
                    \end{align*}

                \end{proof}

                \item[\textbf{d}] If $g(x) \mid f(x)$ and $f(x) \mid g(x)$,
                then $f(x)=kg(x)$ for some $k\in F$.
                \begin{proof}

                    Since $g(x) \mid f(x)$, then by definition $f(x)=q(x)g(x)$,
                    $q(x) \in F[x]$\\
                    And similarly, since $f(x) \mid g(x)$, then by definition
                    $g(x)=r(x)f(x)$, $r(x) \in F[x]$\\
                    Therefore,
                    \begin{align*}
                        \deg(f(x)) & = \deg(q(x)) + \deg(g(x))\\
                        \implies & \deg(f(x)) \le \deg(g(x))
                    \end{align*}
                    And,
                    \begin{align*}
                        \deg(g(x)) & = \deg(r(x)) + \deg(f(x))\\
                        \implies & \deg(g(x)) \le \deg(f(x))
                    \end{align*}
                    Therefore, since $\deg(f(x)) = \deg(g(x))$ \\ and
                    $\deg(q(x)) = \deg(r(x))=0$, then $f(x)=kg(x)$ \qedhere

                \end{proof}

            \end{enumerate}

            \item[\textbf{5}] Over the given field $\mathbb{F}$, write
            $f(x)=q(x)(x-c)+f(c)$ for
            \begin{enumerate}

                \item[\textbf{b}] $f(x)=x^3-5x^2+6x+5$; $c=2$;
                $\mathbb{F}=\mathbb{Q}$;
                \begin{proof}

                    Since $f(x=c=2)=(2)^3-5(2)^2+6(2)+5=5$\\
                    Then,
                    \begin{align*}
                        f(x)-f(c) & = (x^3-5x^2+6x+5)-5\\
                        & = x^3-5x^2+6x\\
                        & = (x^2-3x)(x-2)
                    \end{align*}
                    Therefore, $f(x)=(x^2-3x)(x-2)+5$ \qedhere

                \end{proof}

                \item[\textbf{d}] $f(x)=x^3+2x+3$; $c=2$;
                $\mathbb{F}=\mathbb{Z}_5$;
                \begin{proof}

                    Since $f(x=c=2)=(2)^3+2(2)+3=15 \equiv 0
                    \Mod{\mathbb{Z}_5}$\\
                    Then,
                    \begin{align*}
                        f(x)-f(c) & = (x^3+2x+3)-0\\
                        & = x^3+2x+3\\
                        & = (x^2-x+3)(x+1)
                    \end{align*}
                    Therefore, $f(x)=(x^2-x+3)(x+1)$ \qedhere

                \end{proof}

            \end{enumerate}

            \item[\textbf{6}] Let $p$ be a prime number. Find all roots of
            $x^{p-1}-1$ in $\mathbb{Z}_p$.
            \begin{proof}

                By Fermat's little theorem, for any prime $p$ and $x$ such that
                $p \nmid x$,
                \begin{equation*}
                    x^{p-1}\equiv 1 \Mod{p}
                \end{equation*}
                Since $\{0,1,2,\ldots,p-1\}\in\mathbb{Z}_p$, and there are no
                elements in $\mathbb{Z}_p$ that divides $p$,\\
                then $x^{q-1}\equiv 1 \Mod{p}$ for all $q(\neq
                p)\in\mathbb{Z}_p$
                Therefore, all of $\mathbb{Z}_p$ are roots of the polynomial \qedhere

            \end{proof}

            \item[\textbf{7}] Show that if $c$ is any element of the field
            $\mathbb{F}$ and $k>2$ is an odd integer, then $x+c$ is a factor of
            $x^k+c^k$.
            \begin{proof}

                By the remainder theorem, if $f(x)\in F[x]$ is a non-zero
                polynomial, and $c \in F$, \\ then $\exists\, q(x) \in F[x]$ such
                that $f(x)=q(x)(x-c)+f(c)$\\
                Since $k>2$ is odd, $f(-c)=f(x=-c)=(-c)^k+c^k=0$ \qedhere

            \end{proof}

            \item[\textbf{11}] Show that the set
            $\mathbb{Q}(\sqrt{3})=\{a+b\sqrt{3}\mid a,b \in \mathbb{Q}\}$ is
            closed under addition, subtraction, multiplication, and division.
            \begin{proof}

                Let $x,y \in \mathbb{Q}(\sqrt{3})$, so $x=a_1+b_1\sqrt{3}$,
                $y=a_2+b_2\sqrt{3}$\\
                Addition,
                \begin{align*}
                    x+y & = a_1+b_1\sqrt{3} + a_2+b_2\sqrt{3}\\
                    & = (a_1+a_2) + (b_1+b_2)\sqrt{3}\\
                    & \text{if } c = a_1+a_2, \ d=b_1+b_2 \in \mathbb{Q}\\
                    & \implies c+d\sqrt{3} \in \mathbb{Q}
                \end{align*}
                Subtraction,
                \begin{align*}
                    x-y & = a_1+b_1\sqrt{3} - a_2+b_2\sqrt{3}\\
                    & = (a_1-a_2) + (b_1-b_2)\sqrt{3}\\
                    & \text{if } c = a_1-a_2, \ d=b_1-b_2 \in \mathbb{Q}\\
                    & \implies c+d\sqrt{3} \in \mathbb{Q}
                \end{align*}
                Multiplication,
                \begin{align*}
                    x\cdot y & = (a_1+b_1\sqrt{3}) \cdot (a_2+b_2\sqrt{3})\\
                    & = a_1a_2+a_1b_2\sqrt{3} + b_1\sqrt{3}a_2+b_1\sqrt{3}b_2\sqrt{3}\\
                    & = a_1a_2+a_1b_2\sqrt{3} + b_1a_2\sqrt{3}+3b_1b_2\\
                    & = (a_1a_2+3b_1b_2) + (a_1b_2\sqrt{3} + b_1a_2\sqrt{3})\\
                    & = (a_1a_2+3b_1b_2) + (a_1b_2+b_1a_2)\sqrt{3}\\
                    & \text{if } c = a_1a_2+3b_1b_2, \ d=a_1b_2+b_1a_2 \in \mathbb{Q}\\
                    & \implies c+d\sqrt{3} \in \mathbb{Q}
                \end{align*}
                Division,
                \salign{1}
                \begin{align*}
                    x\div y & = \frac{(a_1+b_1\sqrt{3})}{(a_2+b_2\sqrt{3})}\\
                    & = \frac{(a_1+b_1\sqrt{3})(a_2-b_2\sqrt{3})}{(a_2+b_2\sqrt{3})(a_2-b_2\sqrt{3})}\\
                    & = \frac{(a_1a_2-a_1b_2\sqrt{3}+a_2b_1\sqrt{3}-3b_1b_2)}{(a_2^2-a_2b_2\sqrt{3}+a_2b_2\sqrt{3}+3b_2^2)}\\
                    & = \frac{(a_1a_2-3b_1b_2)+(a_2b_1-a_1b_2)\sqrt{3}}{(a_2^2+3b_2^2)}\\
                    & = \frac{(a_1a_2-3b_1b_2)}{(a_2^2+3b_2^2)}+\frac{(a_2b_1-a_1b_2)}{(a_2^2+3b_2^2)}\sqrt{3}\\
                    & \text{if } c = \frac{(a_1a_2-3b_1b_2)}{(a_2^2+3b_2^2)}, \ d=\frac{(a_2b_1-a_1b_2)}{(a_2^2+3b_2^2)} \in \mathbb{Q}\\
                    & \implies c+d\sqrt{3} \in \mathbb{Q} \qedhere
                \end{align*}
                \endgroup

            \end{proof}

            \item[\textbf{13}] Show that the set of matrices of the form
            $\left[\begin{tabular}{LL}
                        a & b \\
                        -b & a
            \end{tabular}\right]$, where $a,b\in \mathbb{R}$, is a
            field under the operations of matrix addition and
            multiplication.
            \begin{proof}

                Let $A=\left[\begin{tabular}{LL}
                        a & b \\
                        -b & a
                \end{tabular}\right]$, $B=\left[\begin{tabular}{LL}
                        c & d \\
                        -d & c
                \end{tabular}\right] \in M$\\
                To prove closure,
                \salign{0.5}
                \begin{align*}
                    A+B & = \left[\begin{tabular}{LL}
                        a & b \\
                        -b & a
                    \end{tabular}\right]+\left[\begin{tabular}{LL}
                            c & d \\
                            -d & c
                    \end{tabular}\right]\\
                    &=\left[\begin{tabular}{LL}
                            a+c & b+d \\
                            -(b+d) & a+c
                    \end{tabular}\right] \in M
                \end{align*}
                \endgroup
                And,
                \salign{0.5}
                \begin{align*}
                    AB & = \left[\begin{tabular}{LL}
                        a & b \\
                        -b & a
                    \end{tabular}\right]\left[\begin{tabular}{LL}
                            c & d \\
                            -d & c
                    \end{tabular}\right]\\
                    &=\left[\begin{tabular}{LL}
                            ac-bd & ad+bc \\
                            -(bc+ad) & ac-bd
                    \end{tabular}\right] \in M
                \end{align*}
                \endgroup

                To prove that multiplication is commutative,
                \salign{0.5}
                \begin{align*}
                    AB &=\left[\begin{tabular}{LL}
                            ac-bd & ad+bc \\
                            -(bc+ad) & ac-bd
                    \end{tabular}\right] \\
                    & = \left[\begin{tabular}{LL}
                        c & d \\
                        -d & c
                    \end{tabular}\right]\left[\begin{tabular}{LL}
                            a & b \\
                            -b & a
                    \end{tabular}\right]\\
                    & = BA
                \end{align*}
                \endgroup

                To prove the existence of the additive identity element, let $e=\left[\begin{tabular}{LL}
                    0 & 0 \\
                    0 & 0
                \end{tabular}\right]$ such that $A+e=e+A=A$\\
                Since,
                \salign{0.5}
                \begin{align*}
                    A+e &= \left[\begin{tabular}{LL}
                        a & b \\
                        -b & a
                    \end{tabular}\right]+\left[\begin{tabular}{LL}
                            0 & 0 \\
                            0 & 0
                    \end{tabular}\right]\\
                    & = \left[\begin{tabular}{LL}
                        a & b \\
                        -b & a
                    \end{tabular}\right]\\
                    & = A
                \end{align*}
                \endgroup

                To prove the existence of the multiplicative identity element, let $e=\left[\begin{tabular}{LL}
                    1 & 0 \\
                    0 & 1
                \end{tabular}\right]$ such that $Ae=eA=A$\\
                Since,
                \salign{0.5}
                \begin{align*}
                    A+e &= \left[\begin{tabular}{LL}
                        a & b \\
                        -b & a
                    \end{tabular}\right]+\left[\begin{tabular}{LL}
                            1 & 0 \\
                            0 & 1
                    \end{tabular}\right]\\
                    & = \left[\begin{tabular}{LL}
                        a & b \\
                        -b & a
                    \end{tabular}\right]\\
                    & = A
                \end{align*}
                \endgroup

                To prove the existence of additive inverse elements, let $-A=\left[\begin{tabular}{LL}
                    -a & -b \\
                    b & -a
                \end{tabular}\right]$ such that $A+(-A)=(-A)+A=e$\\
                If $A$ is non-zero, then it's determinant $a^2+b^2\neq0$\\
                Therefore
                \salign{0.5}
                \begin{align*}
                    \det{(-A)} &= \frac{1}{aa-(-b)b} \left[\begin{tabular}{LL}
                        -a & -b \\
                        b & -a
                    \end{tabular}\right]\\
                    &= \frac{1}{a^2+b^2} \left[\begin{tabular}{LL}
                        -a & -b \\
                        b & -a
                    \end{tabular}\right] \in M \qedhere
                \end{align*}
                \endgroup

            \end{proof}

            \item[\textbf{17}] Let $(x_0,y_0),(x_1,y_1),(x_2,y_2)$ be points in
            the Euclidean plane $\mathbb{R}^2$ such that $x_0,x_1,x_2$ are
            distinct. Show the formula
            \begin{equation*}
                f(x)=\frac{y_0(x-x_1)(x-x_2)}{(x_0-x_1)(x_0-x_2)}+\frac{y_1(x-x_0)(x-x_2)}{(x_1-x_0)(x_1-x_2)}+\frac{y_2(x-x_0)(x-x_1)}{(x_2-x_0)(x_2-x_1)}
            \end{equation*}
            defines a polynomial $f(x)$ such that $f(x_0)=y_0$, $f(x_1)=y_1$,
            and $f(x_2)=y_2$.
            \begin{proof}

                \salign{1}
                \begin{align*}
                    f(x) &= \frac{y_0(x-x_1)(x-x_2)}{(x_0-x_1)(x_0-x_2)}+\frac{y_1(x-x_0)(x-x_2)}{(x_1-x_0)(x_1-x_2)}+\frac{y_2(x-x_0)(x-x_1)}{(x_2-x_0)(x_2-x_1)}\\
                    & = \frac{\splitfrac{y_0(x-x_1)(x-x_2)(x_1-x_2)}{-y_1(x-x_0)(x-x_2)(x_0-x_2)+y_2(x-x_0)(x-x_1)(x_0-x_1)}}{(x_0-x_1)(x_0-x_2)(x_1-x_2)}\\
                    & = \frac{\splitfrac{y_0(x^2-x_1x-x_2x+x_1x_2)(x_1-x_2)}{-y_1(x^2-x_0x-x_2x+x_0x_2)(x_0-x_2)+y_2(x^2-x_0x-x_1x+x_0x_1)(x_0-x_1)}}{(x_0-x_1)(x_0-x_2)(x_1-x_2)}\\
                    & = \frac{\splitfrac{x^2(y_0(x_0-x_1)-y_1(x_0-x_2)+y_2(x_1-x_2))+x(-y_0(x_0^2-x_1^2)+y_1(x_0^2-x_2^2)-y_2(x_1^2-x_2^2))}{+x_1x_2y_0(x_0-x_1)-x_0x_2y_1(x_0-x_2)+x_0x_1y_2(x_1-x_2))}}{(x_0-x_1)(x_0-x_2)(x_1-x_2)}
                \end{align*}
                \endgroup
                Therefore, $f(x)$ is defined as a polynomial\\
                And,
                \begin{equation*}
                    f(x_0)=\frac{y_0(x_0-x_1)(x_0-x_2)}{(x_0-x_1)(x_0-x_2)}+\frac{y_1(x_0-x_0)(x_0-x_2)}{(x_1-x_0)(x_1-x_2)}+\frac{y_2(x_0-x_0)(x_0-x_1)}{(x_2-x_0)(x_2-x_1)}=y_0
                \end{equation*}
                \begin{equation*}
                    f(x_1)=\frac{y_0(x_1-x_1)(x_1-x_2)}{(x_0-x_1)(x_0-x_2)}+\frac{y_1(x_1-x_0)(x_1-x_2)}{(x_1-x_0)(x_1-x_2)}+\frac{y_2(x_1-x_0)(x_1-x_1)}{(x_2-x_0)(x_2-x_1)}=y_1
                \end{equation*}
                \begin{equation*}
                    f(x_2)=\frac{y_0(x_2-x_1)(x_2-x_2)}{(x_0-x_1)(x_0-x_2)}+\frac{y_1(x_2-x_0)(x_2-x_2)}{(x_1-x_0)(x_1-x_2)}+\frac{y_2(x_2-x_0)(x_2-x_1)}{(x_2-x_0)(x_2-x_1)}=y_2 \qedhere
                \end{equation*}

            \end{proof}

            \item[\textbf{18}] Use Lagrange's interpolation formula to find a
            polynomial $f(x)$ such that $f(1)=0$, $f(2)=1$, and $f(3)=4$.
            \begin{proof}

                Given Lagrange's interpolation formula,
                \begin{equation*}
                    f(x)=\frac{y_0(x-x_1)(x-x_2)}{(x_0-x_1)(x_0-x_2)}+\frac{y_1(x-x_0)(x-x_2)}{(x_1-x_0)(x_1-x_2)}+\frac{y_2(x-x_0)(x-x_1)}{(x_2-x_0)(x_2-x_1)}
                \end{equation*}
                With $x_0=1$, $x_1=2$, and $x_2=3$, and $y_0=0$, $y_1=1$, and $y_2=4$\\
                \begin{align*}
                    f(x)&=\frac{0(x-2)(x-3)}{(1-2)(1-3)}+\frac{1(x-1)(x-3)}{(2-1)(2-3)}+\frac{4(x-1)(x-2)}{(3-1)(3-2)}\\
                    &=-(x-1)(x-3)+2(x-1)(x-2)\\
                    &=x^2-2x+1 \qedhere
                \end{align*}

            \end{proof}

        \end{enumerate}

    \end{itemize}

\end{document}
