{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class 9 - Starter Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import cross_validation\n",
    "from sklearn import grid_search\n",
    "from sklearn import metrics\n",
    "from sklearn import linear_model\n",
    "from sklearn import dummy\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.set(style=\"whitegrid\", font_scale=1)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1. Load Dataset\n",
    "\n",
    "#### 1.1 Let's load the dataset and check the first five rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load data\n",
    "df = pd.read_csv('../../assets/dataset/admissions.csv')\n",
    "\n",
    "# check head\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2. Pre-Process Data\n",
    "\n",
    "#### 2.1 Check and remove missing values\n",
    "**Reading**: Read Pandas docs on handling missing values:\n",
    "[http://pandas.pydata.org/pandas-docs/stable/missing_data.html](http://pandas.pydata.org/pandas-docs/stable/missing_data.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# check for missing values in each column before dropping\n",
    "print \"Missing values:\"\n",
    "print df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# drop missing values if there are any\n",
    "if df.isnull().sum().sum():\n",
    "    print \"There are missing values\"\n",
    "    df = df.dropna()\n",
    "    print \"Missing values dropped\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# check for missing values in each column after dropping\n",
    "print \"Missing values:\"\n",
    "print df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Check and convert all data types to numerical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# check data types\n",
    "print df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get numerical columns\n",
    "num_cols = set(df.dtypes[((df.dtypes==\"int64\")|(df.dtypes==\"float64\"))].index)\n",
    "non_cols = set(df.columns)-num_cols\n",
    "\n",
    "print \"Numerical columns:\"\n",
    "print num_cols\n",
    "print \"Non-numerical columns:\"\n",
    "print non_cols\n",
    "\n",
    "# here all columns are numeric; no need to convert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 Check and create dummy variables for categorical features\n",
    "**Reading**: API Docs for `get_dummies()`:\n",
    "[http://pandas.pydata.org/pandas-docs/stable/generated/pandas.get_dummies.html](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.get_dummies.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create dummy variables for `prestige` feature\n",
    "# this check allows this code to be run multiple times\n",
    "if 'prestige' in df.columns:\n",
    "    # get dummy variables for prestige\n",
    "    df = df.join(pd.get_dummies(df['prestige'], prefix='prestige'))\n",
    "    # remove prestige column\n",
    "    df.drop(['prestige'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# check for newly added columns\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3. Cross Validation\n",
    "\n",
    "#### 3.1 Create separate training and test sets\n",
    "**Reading**:\n",
    "\n",
    "Read Scikit docs on cross validation:\n",
    "[http://scikit-learn.org/stable/modules/cross_validation.html](http://scikit-learn.org/stable/modules/cross_validation.html)\n",
    "\n",
    "Read Scikit docs on `sklearn.cross_validation.train_test_split()`:\n",
    "[http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.train_test_split.html](http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.train_test_split.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# set X and y\n",
    "X = df.drop(['admit'], axis=1)\n",
    "y = df['admit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create separate training and test set with 60/40 train/test split\n",
    "X_train, X_test, y_train, y_test = cross_validation.train_test_split(X, y, test_size=0.4, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# check size of training set\n",
    "print X_train.shape, y_train.shape\n",
    "\n",
    "# check size of test set\n",
    "print X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4. Build Model\n",
    "\n",
    "#### 4.1 Build Logistic Regression Classifier\n",
    "**Reading**: Read Scikit docs for `sklearn.linear_model.LogisticRegression`: [http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# instantiate lm classifier using default params\n",
    "lm = linear_model.### FILL IN ###\n",
    "\n",
    "# train lm classifier on training set\n",
    "lm.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Evaluate Model\n",
    "**Reading**: Read Scikit docs on evaluating models: [http://scikit-learn.org/stable/modules/model_evaluation.html](http://scikit-learn.org/stable/modules/model_evaluation.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# check model accuracy on test set\n",
    "accuracy = lm.score(X_test, y_test)\n",
    "print \"Accuracy: \" + str(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reading**: Read Scikit docs on confusion matrix: [http://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get confusion matrix on test set\n",
    "y_pred = lm.predict(X_test)\n",
    "cm = metrics.confusion_matrix(y_test, y_pred)\n",
    "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "ax = plt.axes()\n",
    "sns.heatmap(cm_normalized, annot=True)\n",
    "ax.set_ylabel('True')\n",
    "ax.set_xlabel('Pred')\n",
    "plt.show()\n",
    "\n",
    "print cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reading**: Read API Docs for [sklearn.metrics.roc_curve](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_curve.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot ROC curve and get AUC score\n",
    "\n",
    "# train and predict using dummy model\n",
    "dm = dummy.DummyClassifier()\n",
    "dm.fit(X_train, y_train)\n",
    "predict_proba_dm_test = dm.predict_proba(X_test).T[1]\n",
    "\n",
    "# train and predict using logstic model\n",
    "lm = linear_model.LogisticRegression()\n",
    "lm.fit(X_train, y_train)\n",
    "predict_proba_lm_test = lm.predict_proba(X_test).T[1]\n",
    "\n",
    "# plot ROC curve\n",
    "ax = plt.subplot(111)\n",
    "vals = metrics.roc_curve(y_test, predict_proba_dm_test)\n",
    "ax.plot(vals[0], vals[1])\n",
    "vals = metrics.### FILL IN ###\n",
    "ax.plot(vals[0], vals[1])\n",
    "\n",
    "ax.set(title='Area Under the Curve for Prediction', ylabel='True Positive Rate', xlabel='False Positive Rate', xlim=(0, 1), ylim=(0, 1))\n",
    "\n",
    "# get AUC score\n",
    "print \"ROC AUC Score: \" + str(metrics.roc_auc_score(y_test, lm.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5. Tune Model\n",
    "\n",
    "#### 5.1 Perform Grid Search for `C`\n",
    "**Reading**: Read Scikit docs for `sklearn.grid_search.GridSearchCV`:\n",
    "[http://scikit-learn.org/stable/modules/generated/sklearn.grid_search.GridSearchCV.html](http://scikit-learn.org/stable/modules/generated/sklearn.grid_search.GridSearchCV.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# set list of values to grid search over\n",
    "c = range(1, 150)\n",
    "params = {'C': c}\n",
    "\n",
    "# perform grid search using list of values\n",
    "gs = grid_search.GridSearchCV(\n",
    "    estimator=linear_model.LogisticRegression(),\n",
    "    param_grid=params)\n",
    "gs.fit(X_train, y_train)\n",
    "\n",
    "# get best value to use\n",
    "print gs.### FILL IN ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plot search values vs. grid scores\n",
    "plt.plot(c, [s[1] for s in gs.grid_scores_])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2 Update model using best `n_neighbors`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# instantiate lm classifier using updated params\n",
    "lm = linear_model.LogisticRegression(### FILL IN ###\n",
    "\n",
    "# train updated lm classifier on training set\n",
    "lm.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3 Evaluate updated model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# check updated model accuracy on test set\n",
    "accuracy = lm.score(X_test, y_test)\n",
    "print \"Accuracy: \" + str(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get confusion matrix on test set\n",
    "y_pred = lm.predict(X_test)\n",
    "cm = metrics.confusion_matrix(y_test, y_pred)\n",
    "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "ax = plt.axes()\n",
    "sns.heatmap(cm_normalized, annot=True)\n",
    "ax.set_ylabel('True')\n",
    "ax.set_xlabel('Pred')\n",
    "plt.show()\n",
    "\n",
    "print cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot ROC curve and get AUC score of updated model\n",
    "\n",
    "# train and predict using dummy model\n",
    "dm = dummy.DummyClassifier()\n",
    "dm.fit(X_train, y_train)\n",
    "predict_proba_dm_test = dm.predict_proba(X_test).T[1]\n",
    "\n",
    "# train and predict using logstic model\n",
    "lm1 = linear_model.LogisticRegression()\n",
    "lm1.fit(X_train, y_train)\n",
    "predict_proba_lm1_test = lm1.predict_proba(X_test).T[1]\n",
    "\n",
    "# train and predict using improved logstic model\n",
    "lm2 = ### FILL IN ###\n",
    "lm2.fit(X_train, y_train)\n",
    "predict_proba_lm2_test = ### FILL IN ###\n",
    "\n",
    "# plot ROC curve\n",
    "ax = plt.subplot(111)\n",
    "vals = metrics.roc_curve(y_test, predict_proba_dm_test)\n",
    "ax.plot(vals[0], vals[1], label='Dummy Model')\n",
    "vals = metrics.roc_curve(y_test, predict_proba_lm1_test)\n",
    "ax.plot(vals[0], vals[1], label='Logistic Regression')\n",
    "vals = ### FILL IN ###\n",
    "ax.plot(vals[0], vals[1], label='Improved Logistic Regression')\n",
    "\n",
    "ax.set(title='Area Under the Curve for Prediction', ylabel='True Positive Rate', xlabel='False Positive Rate', xlim=(0, 1), ylim=(0, 1))\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "# get AUC score\n",
    "print \"ROC AUC Score for lm1: \" + str(metrics.roc_auc_score(y_test, lm1.predict(X_test)))\n",
    "print \"ROC AUC Score for lm2: \" + ### FILL IN ###"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
