{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 4: Starter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, you will combine and apply your knowledge from all three past unit projects to create a complete data science workflow on a new dataset. We will use the Kaggle Titanic competition dataset for this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import cross_validation\n",
    "from sklearn import neighbors\n",
    "from sklearn import grid_search\n",
    "from sklearn import metrics\n",
    "from sklearn import linear_model\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.set(style=\"whitegrid\", font_scale=1)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../assets/images/workflow/data-science-workflow-01.png)\n",
    "\n",
    "## Part 1. Identify the Problem\n",
    "\n",
    "Using the competition description on [Kaggle](https://www.kaggle.com/c/titanic), write a short paragraph summarizing the problem, your goals and hypothesis.\n",
    "\n",
    "**NOTE**: This section can be less rigorous for a kaggle competition where the problem, goals, and hypothesis are identified for you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem**: `*** FILL IN ***`\n",
    "\n",
    "**Goals**: `*** FILL IN ***`\n",
    "\n",
    "**Hypothesis**: `*** FILL IN ***`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../assets/images/workflow/data-science-workflow-02.png)\n",
    "\n",
    "## Part 2. Acquire the Data\n",
    "\n",
    "Kaggle has provided two files for this dataset:  \n",
    "_train.csv_: Use for building a model (contains target variable \"Survived\")  \n",
    "_test.csv_: Use for submission file (fill in for target variable \"Survived\")\n",
    "\n",
    "Both files have been downloaded and added to your datasets folder. Read the files into a Pandas DataFrame.\n",
    "\n",
    "**HINT**: You can further split _train.csv_ to generate your own cross validation set. However, use all of _train.csv_ to train your final model since Kaggle has already separated the test set for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "df = pd.read_csv(\"../assets/datasets/titanic/train.csv\")\n",
    "\n",
    "# Check head\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../assets/images/workflow/data-science-workflow-03-05.png)\n",
    "\n",
    "## Part 3. Parse, Mine, and Refine the data\n",
    "\n",
    "Perform exploratory data analysis and verify the quality of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check columns and counts to drop any non-generic or near-empty columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Check columns\n",
    "*** FILL IN ***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Check counts\n",
    "*** FILL IN ***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for missing values and drop or impute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Check counts for missing values in each column\n",
    "*** FILL IN ***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write a function to wrangle the data to address any issues from above checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def wrangler(df):\n",
    "    # Drop non-generic columns PassengerId, Name, Ticket, and near-empty column Cabin\n",
    "    df = df.drop(['PassengerId', 'Name', 'Ticket', 'Cabin'], axis=1)\n",
    "\n",
    "    # Replace missing values for age using median value\n",
    "    df.loc[(df['Age'].isnull()), 'Age'] = df['Age'].dropna().median()\n",
    "    \n",
    "    # Replace missing values for Fare using median value (there are some missing in Kaggle's test set)\n",
    "    df.loc[(df['Fare'].isnull()), 'Fare'] = *** FILL IN ***\n",
    "    \n",
    "    # Replace missing values for embarked using mode value\n",
    "    df.loc[(df['Embarked'].isnull()), 'Embarked'] = *** FILL IN ***\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Apply wrangler() to DF\n",
    "df = wrangler(df)\n",
    "\n",
    "# Check data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform exploratory data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get summary statistics for data\n",
    "*** FILL IN ***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get pair plot for data\n",
    "*** FILL IN ***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Analyze Unnormalized and Normalized Survival by Sex\n",
    "\n",
    "# Get group by counts for Unnormalized Survival by Sex\n",
    "df_by_sex_unnorm = pd.DataFrame()\n",
    "df_by_sex_unnorm['male'] = df[df['Sex']=='male']['Survived'].value_counts()\n",
    "df_by_sex_unnorm['female'] = df[df['Sex']=='female']['Survived'].value_counts()\n",
    "\n",
    "# Get group by counts for Normalized Survival by Sex\n",
    "df_by_sex_normed = pd.DataFrame()\n",
    "df_by_sex_normed['male'] = df[df['Sex']=='male']['Survived'].value_counts(normalize=True)\n",
    "df_by_sex_normed['female'] = df[df['Sex']=='female']['Survived'].value_counts(normalize=True)\n",
    "\n",
    "# Plot Unnormalized and Normalized Survival by Sex\n",
    "fig = plt.figure(figsize=(10,4))\n",
    "ax1 = fig.add_subplot(121)\n",
    "df_by_sex_unnorm.plot(ax=ax1, kind='bar', rot=0, title=\"Survival by Sex\")\n",
    "ax2 = fig.add_subplot(122)\n",
    "df_by_sex_normed.plot(ax=ax2, kind='bar', rot=0, title=\"Normalized Survival by Sex\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# What other exploratory analysis can you perform?\n",
    "*** FILL IN ***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check and convert all data types to numerical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Check data types\n",
    "*** FILL IN ***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write a function to pre-process data for building a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pre_proc(df):\n",
    "    # Create dummy variables for all non-numerical columns\n",
    "    \n",
    "    # Get dummy variables for Sex\n",
    "    *** FILL IN ***\n",
    "    # Remove Sex column\n",
    "    *** FILL IN ***\n",
    "\n",
    "    # What other columns need dummy variables?\n",
    "    *** FILL IN ***\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Apply pre_proc() to DF\n",
    "df = pre_proc(df)\n",
    "\n",
    "# Check cleaned data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../assets/images/workflow/data-science-workflow-06.png)\n",
    "\n",
    "## Part 4. Build a Model\n",
    "\n",
    "Create a cross validation split, select and build a model, evaluate the model, and refine the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create cross validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Set target variable name\n",
    "target = 'Survived'\n",
    "\n",
    "# Set X and y\n",
    "X = df.drop([target], axis=1)\n",
    "y = df[target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create separate training and test sets with 60/40 train/test split\n",
    "X_train, X_test, y_train, y_test = *** FILL IN ***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Instantiate logistic regression classifier using default params\n",
    "lm = *** FILL IN ***\n",
    "\n",
    "# Train logistic regression classifier on training set\n",
    "*** FILL IN ***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Check model accuracy on test set\n",
    "print \"Accuracy: %0.3f\" % lm.score(*** FILL IN ***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get confusion matrix on test set\n",
    "y_pred = lm.predict(X_test)\n",
    "cm = metrics.confusion_matrix(*** FILL IN ***\n",
    "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "ax = plt.axes()\n",
    "sns.heatmap(cm_normalized, annot=True)\n",
    "ax.set_ylabel('True')\n",
    "ax.set_xlabel('Pred')\n",
    "plt.show()\n",
    "\n",
    "print \"Confusion Matrix:\"\n",
    "print cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot ROC curve and get AUC score\n",
    "y_pred_proba = lm.predict_proba(X_test)[:,1]\n",
    "\n",
    "# Determine the false positive and true positive rates\n",
    "fpr, tpr, t = metrics.roc_curve(y_test, y_pred_proba)\n",
    "\n",
    " \n",
    "# Plot of a ROC curve for a specific class\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr)\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve\")\n",
    "plt.show()\n",
    "\n",
    "# Get ROC AUC score\n",
    "print 'ROC AUC: %0.3f' % metrics.roc_auc_score(y_test, y_pred_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# What other metrics can you calculate?\n",
    "*** FILL IN ***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tune the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Set list of values to grid search over\n",
    "c = [0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
    "s = ['newton-cg', 'lbfgs', 'liblinear', 'sag']\n",
    "params = {'C': c, 'solver':s}\n",
    "\n",
    "# Perform grid search using list of values\n",
    "gs = *** FILL IN ***\n",
    "gs.fit(X_train, y_train)\n",
    "\n",
    "# Get best value to use\n",
    "print \"Best Params:\"\n",
    "print gs.*** FILL IN ***\n",
    "\n",
    "# Get improvement\n",
    "print \"Accuracy of current model: %0.3f\" % lm.score(X_test, y_test)\n",
    "print \"Accuracy using best param: %0.3f\" % gs.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Current model params\n",
    "print lm\n",
    "print \"Accuracy of current model: %0.3f\" % lm.score(X_test, y_test)\n",
    "\n",
    "# Update model params\n",
    "lm.set_params(C=*** FILL IN ***\n",
    "lm.set_params(solver=*** FILL IN ***\n",
    "\n",
    "# Retrain model on new params\n",
    "lm.fit(X_train, y_train)\n",
    "\n",
    "# Updated model params\n",
    "print lm\n",
    "print \"Accuracy of updated model: %0.3f\" % lm.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../assets/images/workflow/data-science-workflow-07.png)\n",
    "\n",
    "## Part 5: Present the Results\n",
    "\n",
    "Generate summary of findings and kaggle submission file.\n",
    "\n",
    "NOTE: For the purposes of generating summary narratives and kaggle submission, we can train the model on the entire training data provided in _train.csv_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Kaggle training data and use entire data to train tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "df_train = pd.read_csv(\"../assets/datasets/titanic/train.csv\")\n",
    "\n",
    "# Wrangle data\n",
    "df_train = wrangler(df_train)\n",
    "\n",
    "# Pre-process data\n",
    "df_train = pre_proc(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set target variable name\n",
    "target = 'Survived'\n",
    "\n",
    "# Set X_train and y_train\n",
    "X_train = df_train.drop([target], axis=1)\n",
    "y_train = df_train[target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Build tuned model\n",
    "lm = linear_model.LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
    "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
    "          penalty='l2', random_state=None, solver='newton-cg', tol=0.0001,\n",
    "          verbose=0, warm_start=False)\n",
    "\n",
    "# Train tuned model\n",
    "lm.fit(X_train, y_train)\n",
    "\n",
    "# Score tuned model\n",
    "print \"Accuracy: %0.3f\" % lm.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use trained model to generate a few summary findings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Generate probabililty of survivial using trained model\n",
    "df_train['Probability'] = lm.predict_proba(X_train)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot Probability of Survivial Based on Sex and Age\n",
    "ax = df_train[df_train['Sex_male']==1].plot(x='Age', y='Probability', kind='scatter', color='b', label='Male')\n",
    "df_train[df_train['Sex_female']==1].plot(ax=ax, x='Age', y='Probability', kind='scatter', color='m', label='Female')\n",
    "ax.set(title='Probability of Survival\\n Based on Sex and Age')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot Probability of Survivial Based on Pclass and Age\n",
    "sns.lmplot(x=\"Age\", y=\"Probability\", hue=\"Pclass\", data=df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# What other summary visualizations can you make?\n",
    "*** FILL IN ***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Summary of findings**: `*** FILL IN ***`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Kaggle test data, make predictions using model, and generate submission file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "df_test = pd.read_csv(\"../assets/datasets/titanic/test.csv\")\n",
    "\n",
    "# Create DF for submission\n",
    "df_sub = df_test[['PassengerId']]\n",
    "\n",
    "# Wrangle data\n",
    "df_test = wrangler(df_test)\n",
    "\n",
    "# Pre-process data\n",
    "df_test = pre_proc(df_test)\n",
    "\n",
    "# Check data\n",
    "df_sub.head()\n",
    "\n",
    "# Predict using tuned model\n",
    "df_sub['Survived'] = lm.predict(df_test)\n",
    "\n",
    "# Write submission file\n",
    "df_sub.to_csv(\"../assets/datasets/titanic/mysubmission.csv\", index=False)\n",
    "\n",
    "print \"Check ../assets/datasets/titanic/ for submission file\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Kaggle score** _(if submitted to Kaggle)_: `*** FILL IN ***`\n",
    "\n",
    "**HINT**: Try tranforming or combining features and create additional features to improve your score. This is a popular introductory dataset, Google for additional feature engineering hints!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
