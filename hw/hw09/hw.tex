%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% Sabbir Ahmed
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass[paper=usletter, fontsize=12pt]{article}
\input{../structure.tex} % specifies the document layout and style

\begin{document}

    \documentinfo{\today}{09}

    \begin{enumerate}

        \item Let $X$ be a random variable with PDF $f_X$. Find the PDF of the
        random variable $|X|$ in the following three cases.
        \begin{enumerate}

            \item $X$ is exponentially distributed with parameter $\lambda$.
            \begin{proof}

                Given, $f_X=\lambda e^{-\lambda x}$
                \begin{align*}
                    \int_{-\infty}^{\infty}f_X(x)\diff{x} & = \int_{-\infty}^{0}\lambda e^{-\lambda x}\diff{x} + \int_{0}^{\infty}\lambda e^{-\lambda x}\diff{x} \\
                    & = \int_{0}^{\infty}\lambda e^{-\lambda x}\diff{x} + \int_{0}^{\infty}\lambda e^{-\lambda x}\diff{x} \\
                    & = 2
                \end{align*}
                Therefore, $f_X(x) = \frac{1}{2}\lambda e^{-\lambda |x|}$ \qedhere

            \end{proof}

            \item $X$ is uniformly distributed in the interval $[-1,2]$.
            \begin{proof}

                Given, $f_X$ is uniform between $[-1,2]$,
                \salign{0.7}
                \begin{align*}
                    f_X(x) & = \frac{1}{(b-a)} \\
                    & = \frac{1}{(2 - (-1))} \\
                    & = \frac{1}{3} \qedhere
                \end{align*}
                \endgroup

            \end{proof}

            \item $f_X$ is a general PDF.
            \begin{proof}

                \begin{align*}
                    \int_{\infty}^{\infty}f_X(x)\diff{x} & = 1 \\
                    \implies 2\int_{0}^{\infty}f_X(x)\diff{x} & = 1 \\
                    \implies \int_{0}^{\infty}f_X(x)\diff{x} & = \frac{1}{2} \qedhere
                \end{align*}

            \end{proof}

        \end{enumerate}

        \item Let $X$ and $Y$ be independent random variables, uniformly
        distributed in the interval $[0,1]$. Find the CDF and the PDF of
        $|X-Y|$.
        \begin{proof}
        \end{proof}

        \item Your driving time to work is between 30 and 45 minutes if the day
        is sunny, and between 40 and 60 minutes if the day is rainy, with all
        times being equally likely in each case. Assume that a day is sunny
        with probability 2/3 and rainy with probability 1/3.
        \begin{enumerate}

            \item Find the PDF, the mean, and the variance of your driving
            time.
            \begin{proof}

                Given, $P(\text{sunny}) = \frac{2}{3}$, $P(\text{rainy}) =
                \frac{1}{3}$\\

                PDF of uniform distribution
                \salign{1}
                \begin{align*}
                    f_X(x) & =
                    \begin{dcases}
                        c_1,  & \text{if } 30 \le x \le 40,\\
                        c_2,  & \text{if } 40 \le x \le 45,\\
                        c_3,  & \text{if } 45 \le x \le 60,\\
                        0,    & \text{otherwise}
                    \end{dcases}\\
                    & = \begin{dcases}
                        \frac{1}{15} \cdot P(\text{sunny}), & \text{if } 30 \le x \le 40,\\
                        \frac{1}{15} \cdot P(\text{sunny}) + \frac{1}{20} \cdot P(\text{rainy}), & \text{if } 40 \le x \le 45,\\
                        \frac{1}{20} \cdot P(\text{rainy}), & \text{if } 45 \le x \le 60,\\
                        0,    & \text{otherwise}
                    \end{dcases}\\
                    & =\begin{dcases}
                        \frac{2}{45}, & \text{if } 30 \le x \le 40,\\
                        \frac{11}{180}, & \text{if } 40 \le x \le 45,\\
                        \frac{1}{80}, & \text{if } 45 \le x \le 60,\\
                        0,    & \text{otherwise}
                    \end{dcases}
                \end{align*}
                \endgroup
                Mean of uniform distribution
                \salign{0.7}
                \begin{align*}
                    E[X] & = \frac{30+45}{2} \cdot \frac{2}{3} + \frac{40+60}{2} \cdot \frac{1}{3}\\
                    & = \frac{125}{3}
                \end{align*}
                \endgroup

                Variance of uniform distribution
                \salign{0.7}
                \begin{align*}
                    E[X^2] & = \frac{2}{45}\int_{30}^{45}x^2\diff{x} + \frac{1}{60}\int_{40}^{60}x^2\diff{x}\\
                    & = \frac{16150}{9}\\
                    var(X) & = E[X^2] - (E[X])^2\\
                    & = \frac{175}{3}  \qedhere
                \end{align*}
                \endgroup

            \end{proof}

            \item On a given day your driving time was 45 minutes. What is the
            probability that this particular day was rainy?
            \begin{proof}

                \begin{align*}
                    P(\text{rainy} \mid X=45) & = \frac{P(\text{rainy})f(X=45 \mid \text{rainy})}{f(X=45)}\\
                    & = \frac{\frac{1}{3}\cdot\frac{1}{20}}{\frac{2}{45}+\frac{1}{60}} \\
                    & = \frac{3}{11} \qedhere
                \end{align*}

            \end{proof}

            \item Your distance to work is 20 miles. What is the PDF, the mean,
            and the variance of your average speed (driving distance over
            driving time)?
            \begin{proof}



            \end{proof}

        \end{enumerate}

        \item The random variables $X$ and $Y$ have the join PDF
        \begin{align*}
            f_{X,Y}(x,y) & =
            \begin{dcases}
                2, & \text{if $x > 0$ and $y>0$ and $x+y\le 1$},\\
                0, & \text{otherwise}
            \end{dcases}
        \end{align*}
        Let $A$ be the event $\{Y \le 0.5\}$ and let $B$ be the event
        $\{Y>X\}$.
        \begin{enumerate}

            \item Calculate $P(B \mid A)$.
            \begin{proof}

                Since $P(B \mid A) = \frac{P(A \cap B)}{P(A)}$,
                \salign{1}
                \begin{align*}
                    P(A \cap B) & = \int_{0}^{0.5}\int_{0}^{1-y}2\diff{x}\diff{y} \\
                    & = \frac{3}{4} \\
                    P(A) & = \int_{0}^{0.5}\int_{x}^{0.5}2\diff{y}\diff{x} \\
                    & = \frac{1}{4}
                \end{align*}
                \endgroup
                Therefore,
                \begin{align*}
                    P(B \mid A) & = \frac{1}{3} \qedhere
                \end{align*}

            \end{proof}

            \item Calculate $f_{X \mid Y}(x \mid 0.5)$. Calculate also the
            conditional expectation and the conditional variance of $X$, given
            that $Y=0.5$.
            \begin{proof}

                Since $f_{X \mid Y}(x \mid y) = \frac{f_{X,Y}(x,y)}{f_Y(y)}$,
                \salign{1}
                \begin{align*}
                    f(y) &= \int_{0}^{1-y}2\diff{x}\\
                    & = 2-2y \\
                    f_{X \mid Y}(x \mid 0.5) & = \frac{2}{(2-2(0.5))} \\
                    & = 2
                \end{align*}
                \endgroup
                And,
                \salign{1}
                \begin{align*}
                    E[X \mid Y = 0.5] &= \int_{0}^{0.5}2x\diff{x}\\
                    & = \frac{1}{4} \\
                    E[X^2 \mid Y = 0.5] &= \int_{0}^{0.5}2x^2\diff{x}\\
                    & = \frac{1}{12} \\
                    \var{X \mid Y = 0.5} & = \frac{1}{12} - \bigg(\frac{1}{4}\bigg)^2\\
                    & = \frac{1}{48} \qedhere
                \end{align*}
                \endgroup

            \end{proof}

            \item Calculate $f_{X \mid B}(x)$.
            \begin{proof}

                Since $f_{X \mid B}(x) = \frac{f(X \cap B)}{f(B)}$,
                \salign{1}
                \begin{align*}
                    f(X \cap B) & = \int_{x}^{1-x}2\diff{y} \\
                    & = 2 - 4x \\
                    f(B) & = \int_{0}^{1/2}\int_{x}^{1-x}2\diff{x}\diff{y} \\
                    & = \frac{1}{2} \\
                    f_{X \mid B}(x) & = \frac{2 - 4x}{\frac{1}{2}} \\
                    & = 4 - 8x, \ 0 \le x \le \frac{1}{2} \qedhere
                \end{align*}
                \endgroup

            \end{proof}

            \item Calculate $E[XY]$.
            \begin{proof}

                \salign{0.7}
                \begin{align*}
                    E[XY] & = \int_{0}^{1}\int_{0}^{1-y}2xy\diff{y}\diff{x}\\
                    & = \frac{1}{12} \qedhere
                \end{align*}
                \endgroup

            \end{proof}

            \item Calculate the PDF of $Y/X$.
            \begin{proof}
            \end{proof}

        \end{enumerate}

        \item The random variables $X_1,\ldots,X_n$ have common mean $\mu$,
        common variance $\sigma^2$ and, furthermore, $E[X_iX_j=c]$ for every
        pair of distinct $i$ and $j$. Derive a formula for the variance
        $X_1+\ldots+X_n$, in terms of $\mu$, $\sigma^2$, $c$, and $n$.
        \begin{proof}

            Given,
            \begin{equation*}
                E[x_i] = \mu, \  var(x_i) = \sigma^2, \text{for } 1 \le i \le n
            \end{equation*}
            Also, given for every pair $i$ and $j$, $E[X_iX_j]=c$\\
            Then,
            \begin{align*}
                cov(X_iX_j) &= E[X_iX_j] - E[X_i]E[X_j]\\
                & = c - \mu \cdot \mu\\
                & = c - \mu^2
            \end{align*}
            \salign{1}
            \begin{align*}
                var(X_1+\ldots+X_n) &= var(X_1) + var(X_2) + \ldots + var(X_n) + 2cov(X_1,X_2) + 2cov(X_2,X_3)+\ldots+ 2cov(X_{n-1},X_{n}) \\
                & = \sum_{i=1}^{n}var(X_i)+\sum_{i=1}^{n-1}\sum_{j=2}^{n}2cov(X_i,X_j)\\
                & = \sum_{i=1}^{n}\sigma^2 + \sum_{i=1}^{n-1}\sum_{j=2}^{n}2(c-\mu^2) \\
                & = n\sigma^2 + (n-1)2(c-\mu^2) \\
                & = n\sigma^2 + 2(n-1)(c-\mu^2)  \qedhere
            \end{align*}
            \endgroup

        \end{proof}

        \item Consider $n$ independent tosses of a die. Each toss has
        probability $p_i$ of resulting in $i$. Let $X_i$ be the number of
        tosses that result in $i$. Show that $X_1$ and $X_2$ are negatively
        correlated (i.e., a large number of ones suggests a smaller number of
        twos).
        \begin{proof}

            Given $P(x_k=i)=p_i$ for $i=1,2,3,4,5,6$ and $k=1,2,\ldots,n$ \\ Then,
            \begin{equation*}
                X_i = \sum_{k=1}^n 1_{\{x_k=i\}}
            \end{equation*}
            Thus,
            \salign{1}
            \begin{align*}
                \cov{X_1,X_2} &= E[X_1X_2]-E[X_1]E[X_2]\\
                &= E\left[\left(\sum_{k=1}^n1_{\{x_k=1\}}\right)\left(\sum_{k=1}^n1_{\{x_k=2\}}\right)\right] - (np_1)(np_2).\\
            \end{align*}
            \endgroup
            Since, $1_{\{x_i=1\}}1_{\{x_i=2\}}=0$,\\
            then $E\left[1_{\{x_i=1\}}1_{\{x_i=2\}}\right]=0$
            \begin{align*}
                E\left[1_{\{x_i=1\}}1_{\{x_j=2\}}\right] &= P(x_i=1,x_j=2)\\
                & =P(x_i=1)P(x_j=2) \\
                &= p_1p_2, \text{for $i\ne j$}
            \end{align*}
            Therefore,
            \begin{align*}
                \cov{X_1,X_2} &= (n^2-n)p_1p_2 -n^2p_1p_2 \\
                & =-np_1p_2 <0 \qedhere
            \end{align*}

        \end{proof}

        \item Let $X=Y-Z$ where $Y$ and $Z$ are nonnegative random variables
        such that $YZ=0$.
        \begin{enumerate}

            \item Show that $\cov{Y,Z} \le 0$.
            \begin{proof}

                \begin{align*}
                    \cov{Y, Z} &= E[(Y - E(Y))(Z - E(Z))]\\
                    &=E[YZ-2Y\cdot E[Z]-E[Z]^2] \\
                    &=E[YZ]-2\cdot E[Y]\cdot E[Z]-E[Z]^2 \\
                    &=-2\cdot E[Y]\cdot E[Z]-E[Z]^2 < 0  \qedhere
                \end{align*}

            \end{proof}

            \item Show that $var(X) \ge var(Y)+var(Z)$.
            \begin{proof}

                Since $X=Y-Z$,
                \begin{align*}
                    \var{X} &=\var{Y-Z} \\
                    &= \var{Y}+\var{Z}+2 \cdot\cov{Y,Z}
                \end{align*}
                But since $\cov{Y,Z}< 0$, \\
                Then, $\var{X} >=\var{Y}+\var{Z}$ \qedhere

            \end{proof}

            \item Use the result of part (b) to show that
            \begin{equation*}
                var(X) \ge var(\max\{0,X\})+var(\max\{0,-X\})
            \end{equation*}
            \begin{proof}

                If $Y>Z$, then $\max{0,X}=X$ and $\max{0,-X}=0$\\
                Therefore,
                \begin{align*}
                    \var{\max{0,X}}+\var{\max{0,-X}} &=\var{X}+\var{0}\\
                    & =\var{X}
                \end{align*}

                If $Y<Z$, then $\max{0,X}=0$ and $\max{0,-X}=-X$\\
                Therefore
                \begin{align*}
                    \var{\max{0,X}}+\var{\max{0,-X}} &=\var{0}+\var{-X}\\
                    & =\var{X}
                \end{align*}

                If $Y=0$, $Z=0$,\\
                then $\max{0,X}=X$ and $\max{0,-X}=0$\\
                Therefore,
                \begin{align*}
                    \var{\max{0,X}}+\var{\max{0,-X}} &=\var{0}+\var{0} \\
                    & =0
                \end{align*}

                Therefore, $\var{X} \ge \var{\max\{0,X\}}+\var{\max\{0,-X\}}$
                \qedhere

            \end{proof}

        \end{enumerate}

        \item Consider two random variables $X$ and $Y$. Assume for simplicity
        that they both have zero mean.
        \begin{enumerate}

            \item Show that $X$ and $E[X\mid Y]$ are positively correlated.
            \begin{proof}

                Given $E[X]=0$, $E[Y]=0$\\
                Then,
                \begin{align*}
                    E[X\cdot E[X\mid Y]] & = E[E[X\cdot E[X\mid Y] \mid Y]] \\
                    & = E[E[X\mid Y]\cdot E[X\mid Y]] \\
                    & = E[E^2[X\mid Y]] > 0
                \end{align*}
                Since $E[E[X\mid Y]]=E[X]=0$,\\
                $\cov{X,E[X\mid Y]} = E[E^2[X\mid Y]] > 0$ \qedhere

            \end{proof}

            \item Show that the correlation coefficient of $Y$ and $E[X\mid Y]$
            has the same sign as the correlation coefficient of $X$ and $Y$.
            \begin{proof}

                \begin{align*}
                    \cov{Y,E[X\mid Y]} & = E[Y\cdot E[X\mid Y]] \\
                    \implies \cov{X,Y} & = E[XY]
                \end{align*}
                Since,
                \begin{align*}
                    E[XY] & = E[E[XY\mid Y]] \\
                    & = E[Y\cdot E[X\mid Y]]\\
                    \implies \cov{X,Y} & = \cov{Y,E[X\mid Y]} \qedhere
                \end{align*}

            \end{proof}

        \end{enumerate}

    \end{enumerate}

\end{document}
